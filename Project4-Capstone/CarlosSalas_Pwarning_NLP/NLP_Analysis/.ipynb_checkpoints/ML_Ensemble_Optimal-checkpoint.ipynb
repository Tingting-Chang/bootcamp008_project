{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Optimal Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Note: import new json with new variables calculated below in order to save time:\n",
    "data_new_var=pd.read_json('data_new_var.json')\n",
    "data = data_new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Num_syl</th>\n",
       "      <th>Num_words</th>\n",
       "      <th>Num_sent</th>\n",
       "      <th>Padding</th>\n",
       "      <th>FE_idx</th>\n",
       "      <th>DC_idx</th>\n",
       "      <th>DC_dif_words</th>\n",
       "      <th>Smog_Grade_idx</th>\n",
       "      <th>FK_Grade_idx</th>\n",
       "      <th>...</th>\n",
       "      <th>Padding_QA</th>\n",
       "      <th>FE_idx_QA</th>\n",
       "      <th>DC_idx_QA</th>\n",
       "      <th>DC_dif_words_QA</th>\n",
       "      <th>Smog_Grade_idx_QA</th>\n",
       "      <th>FK_Grade_idx_QA</th>\n",
       "      <th>CL_Grade_idx_QA</th>\n",
       "      <th>ARI_Grade_idx_QA</th>\n",
       "      <th>LW_Grade_idx_QA</th>\n",
       "      <th>GFox_Grade_idx_QA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56774.032258</td>\n",
       "      <td>14144.990323</td>\n",
       "      <td>9730.645161</td>\n",
       "      <td>548.268817</td>\n",
       "      <td>0.483978</td>\n",
       "      <td>34.039032</td>\n",
       "      <td>6.433118</td>\n",
       "      <td>1138.978495</td>\n",
       "      <td>4.877419</td>\n",
       "      <td>8.365591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508065</td>\n",
       "      <td>31.753656</td>\n",
       "      <td>6.553011</td>\n",
       "      <td>695.559140</td>\n",
       "      <td>3.618280</td>\n",
       "      <td>8.049462</td>\n",
       "      <td>9.966882</td>\n",
       "      <td>9.860215</td>\n",
       "      <td>5.967742</td>\n",
       "      <td>9.367742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21679.058099</td>\n",
       "      <td>5496.045418</td>\n",
       "      <td>3848.290430</td>\n",
       "      <td>183.467752</td>\n",
       "      <td>0.026133</td>\n",
       "      <td>6.149230</td>\n",
       "      <td>0.384029</td>\n",
       "      <td>305.182803</td>\n",
       "      <td>2.654779</td>\n",
       "      <td>1.351931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036184</td>\n",
       "      <td>13.947675</td>\n",
       "      <td>1.389671</td>\n",
       "      <td>197.664651</td>\n",
       "      <td>1.735308</td>\n",
       "      <td>1.881409</td>\n",
       "      <td>3.695740</td>\n",
       "      <td>3.504970</td>\n",
       "      <td>2.319350</td>\n",
       "      <td>3.697860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6473.000000</td>\n",
       "      <td>1649.700000</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>23.780000</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>21.410000</td>\n",
       "      <td>5.890000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.230000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49789.000000</td>\n",
       "      <td>12310.200000</td>\n",
       "      <td>8449.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>28.860000</td>\n",
       "      <td>6.220000</td>\n",
       "      <td>1011.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>27.840000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>619.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>9.160000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56398.000000</td>\n",
       "      <td>14196.600000</td>\n",
       "      <td>9746.000000</td>\n",
       "      <td>553.000000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>34.950000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1142.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>29.870000</td>\n",
       "      <td>6.380000</td>\n",
       "      <td>698.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>9.520000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>63344.000000</td>\n",
       "      <td>15813.900000</td>\n",
       "      <td>10655.000000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>37.320000</td>\n",
       "      <td>6.560000</td>\n",
       "      <td>1272.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>32.920000</td>\n",
       "      <td>6.580000</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>9.990000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>210592.000000</td>\n",
       "      <td>53848.800000</td>\n",
       "      <td>37412.000000</td>\n",
       "      <td>1583.000000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>51.530000</td>\n",
       "      <td>8.920000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>157.450000</td>\n",
       "      <td>19.530000</td>\n",
       "      <td>1147.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>42.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Size       Num_syl     Num_words     Num_sent    Padding  \\\n",
       "count      93.000000     93.000000     93.000000    93.000000  93.000000   \n",
       "mean    56774.032258  14144.990323   9730.645161   548.268817   0.483978   \n",
       "std     21679.058099   5496.045418   3848.290430   183.467752   0.026133   \n",
       "min      6473.000000   1649.700000   1031.000000    51.000000   0.420000   \n",
       "25%     49789.000000  12310.200000   8449.000000   477.000000   0.460000   \n",
       "50%     56398.000000  14196.600000   9746.000000   553.000000   0.480000   \n",
       "75%     63344.000000  15813.900000  10655.000000   633.000000   0.500000   \n",
       "max    210592.000000  53848.800000  37412.000000  1583.000000   0.550000   \n",
       "\n",
       "          FE_idx     DC_idx  DC_dif_words  Smog_Grade_idx  FK_Grade_idx  \\\n",
       "count  93.000000  93.000000     93.000000       93.000000     93.000000   \n",
       "mean   34.039032   6.433118   1138.978495        4.877419      8.365591   \n",
       "std     6.149230   0.384029    305.182803        2.654779      1.351931   \n",
       "min    23.780000   5.850000    280.000000        3.100000      5.600000   \n",
       "25%    28.860000   6.220000   1011.000000        3.100000      7.600000   \n",
       "50%    34.950000   6.400000   1142.000000        3.100000      8.400000   \n",
       "75%    37.320000   6.560000   1272.000000        8.800000      9.100000   \n",
       "max    51.530000   8.920000   2998.000000        8.800000     14.200000   \n",
       "\n",
       "             ...          Padding_QA   FE_idx_QA  DC_idx_QA  DC_dif_words_QA  \\\n",
       "count        ...           93.000000   93.000000  93.000000        93.000000   \n",
       "mean         ...            0.508065   31.753656   6.553011       695.559140   \n",
       "std          ...            0.036184   13.947675   1.389671       197.664651   \n",
       "min          ...            0.250000   21.410000   5.890000         2.000000   \n",
       "25%          ...            0.490000   27.840000   6.210000       619.000000   \n",
       "50%          ...            0.510000   29.870000   6.380000       698.000000   \n",
       "75%          ...            0.530000   32.920000   6.580000       806.000000   \n",
       "max          ...            0.560000  157.450000  19.530000      1147.000000   \n",
       "\n",
       "       Smog_Grade_idx_QA  FK_Grade_idx_QA  CL_Grade_idx_QA  ARI_Grade_idx_QA  \\\n",
       "count          93.000000        93.000000        93.000000         93.000000   \n",
       "mean            3.618280         8.049462         9.966882          9.860215   \n",
       "std             1.735308         1.881409         3.695740          3.504970   \n",
       "min             0.000000         6.000000         8.230000          7.000000   \n",
       "25%             3.100000         7.200000         9.160000          8.600000   \n",
       "50%             3.100000         7.900000         9.520000          9.400000   \n",
       "75%             3.100000         8.400000         9.990000         10.100000   \n",
       "max             8.800000        21.800000        44.800000         40.800000   \n",
       "\n",
       "       LW_Grade_idx_QA  GFox_Grade_idx_QA  \n",
       "count        93.000000          93.000000  \n",
       "mean          5.967742           9.367742  \n",
       "std           2.319350           3.697860  \n",
       "min           1.000000           7.200000  \n",
       "25%           4.500000           8.400000  \n",
       "50%           5.500000           8.800000  \n",
       "75%           7.000000           9.600000  \n",
       "max          18.000000          42.800000  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_gr: Get only readability predictors:\n",
    "data_gr= data[[ 'Size', 'Num_syl', 'Num_words', 'Num_sent', 'Padding', \n",
    "               'FE_idx', 'DC_idx', 'DC_dif_words', 'Smog_Grade_idx',\n",
    "               'FK_Grade_idx', 'CL_Grade_idx', 'ARI_Grade_idx', 'LW_Grade_idx', 'GFox_Grade_idx', 'Num_syl_MD', \n",
    "               'Size_MD','Num_words_MD', 'Num_sent_MD', 'Padding_MD', 'FE_idx_MD', 'DC_idx_MD', 'DC_dif_words_MD', \n",
    "               'Smog_Grade_idx_MD', 'FK_Grade_idx_MD', 'CL_Grade_idx_MD', 'ARI_Grade_idx_MD', 'LW_Grade_idx_MD', \n",
    "               'GFox_Grade_idx_MD', 'Num_syl_QA','Size_QA', 'Num_words_QA', 'Num_sent_QA', 'Padding_QA', 'FE_idx_QA', \n",
    "               'DC_idx_QA', 'DC_dif_words_QA', 'Smog_Grade_idx_QA', 'FK_Grade_idx_QA', 'CL_Grade_idx_QA', \n",
    "               'ARI_Grade_idx_QA', 'LW_Grade_idx_QA', 'GFox_Grade_idx_QA']]\n",
    "len(data_gr.columns) # 42 numerical predictors\n",
    "data_gr.describe() # depending what ML method use you might to scale first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlo\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2762: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Polarity_QA</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Subjectivity_QA</th>\n",
       "      <th>neg_perc_abs</th>\n",
       "      <th>neg_perc_rel</th>\n",
       "      <th>pos_perc_abs</th>\n",
       "      <th>pos_perc_rel</th>\n",
       "      <th>unc_perc_abs</th>\n",
       "      <th>unc_perc_rel</th>\n",
       "      <th>...</th>\n",
       "      <th>v_neg_MD</th>\n",
       "      <th>v_neg_QA</th>\n",
       "      <th>v_neu</th>\n",
       "      <th>v_neu_MD</th>\n",
       "      <th>v_neu_QA</th>\n",
       "      <th>v_pos</th>\n",
       "      <th>v_pos_MD</th>\n",
       "      <th>v_pos_QA</th>\n",
       "      <th>w_mod_perc_abs</th>\n",
       "      <th>w_mod_perc_rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.094675</td>\n",
       "      <td>-0.047669</td>\n",
       "      <td>0.068451</td>\n",
       "      <td>0.069818</td>\n",
       "      <td>0.018872</td>\n",
       "      <td>0.187972</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>0.286304</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054794</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.879199</td>\n",
       "      <td>0.884357</td>\n",
       "      <td>0.879257</td>\n",
       "      <td>0.213371</td>\n",
       "      <td>0.153589</td>\n",
       "      <td>0.245192</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.072351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.147835</td>\n",
       "      <td>0.202291</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.046538</td>\n",
       "      <td>0.004658</td>\n",
       "      <td>0.043206</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.046664</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014699</td>\n",
       "      <td>0.041173</td>\n",
       "      <td>0.027835</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>0.038059</td>\n",
       "      <td>0.049806</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.068170</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.018539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.227848</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.049780</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.177453</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.023455</td>\n",
       "      <td>0.778887</td>\n",
       "      <td>0.836053</td>\n",
       "      <td>0.762396</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.108308</td>\n",
       "      <td>0.091524</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.034301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.010830</td>\n",
       "      <td>-0.159091</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>0.057247</td>\n",
       "      <td>0.015445</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.025709</td>\n",
       "      <td>0.262821</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.127358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044750</td>\n",
       "      <td>0.046639</td>\n",
       "      <td>0.860199</td>\n",
       "      <td>0.873145</td>\n",
       "      <td>0.857936</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>0.138208</td>\n",
       "      <td>0.199177</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.057812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.092784</td>\n",
       "      <td>-0.051163</td>\n",
       "      <td>0.066434</td>\n",
       "      <td>0.062940</td>\n",
       "      <td>0.018042</td>\n",
       "      <td>0.182830</td>\n",
       "      <td>0.028696</td>\n",
       "      <td>0.283159</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.151852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053143</td>\n",
       "      <td>0.054647</td>\n",
       "      <td>0.880331</td>\n",
       "      <td>0.885531</td>\n",
       "      <td>0.875645</td>\n",
       "      <td>0.214071</td>\n",
       "      <td>0.153585</td>\n",
       "      <td>0.247348</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.071942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>0.073845</td>\n",
       "      <td>0.071634</td>\n",
       "      <td>0.021394</td>\n",
       "      <td>0.219643</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>0.314935</td>\n",
       "      <td>0.018049</td>\n",
       "      <td>0.175497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.068679</td>\n",
       "      <td>0.898970</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>0.901714</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>0.166529</td>\n",
       "      <td>0.300054</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.084071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.478927</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.030227</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.046953</td>\n",
       "      <td>0.429072</td>\n",
       "      <td>0.037829</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.318333</td>\n",
       "      <td>0.934839</td>\n",
       "      <td>0.917956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317834</td>\n",
       "      <td>0.212932</td>\n",
       "      <td>0.360683</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.129657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Polarity  Polarity_QA  Subjectivity  Subjectivity_QA  neg_perc_abs  \\\n",
       "count  93.000000    93.000000     93.000000        93.000000     93.000000   \n",
       "mean    0.094675    -0.047669      0.068451         0.069818      0.018872   \n",
       "std     0.147835     0.202291      0.009777         0.046538      0.004658   \n",
       "min    -0.227848    -0.999999      0.049780         0.042358      0.011325   \n",
       "25%    -0.010830    -0.159091      0.062305         0.057247      0.015445   \n",
       "50%     0.092784    -0.051163      0.066434         0.062940      0.018042   \n",
       "75%     0.211268     0.070968      0.073845         0.071634      0.021394   \n",
       "max     0.478927     0.454545      0.101510         0.500000      0.030227   \n",
       "\n",
       "       neg_perc_rel  pos_perc_abs  pos_perc_rel  unc_perc_abs  unc_perc_rel  \\\n",
       "count     93.000000     93.000000     93.000000     93.000000     93.000000   \n",
       "mean       0.187972      0.028705      0.286304      0.015562      0.153723   \n",
       "std        0.043206      0.005006      0.046664      0.004245      0.034157   \n",
       "min        0.110333      0.017044      0.177453      0.007700      0.088235   \n",
       "25%        0.152244      0.025709      0.262821      0.012478      0.127358   \n",
       "50%        0.182830      0.028696      0.283159      0.015212      0.151852   \n",
       "75%        0.219643      0.031618      0.314935      0.018049      0.175497   \n",
       "max        0.297170      0.046953      0.429072      0.037829      0.343284   \n",
       "\n",
       "            ...         v_neg_MD   v_neg_QA      v_neu   v_neu_MD   v_neu_QA  \\\n",
       "count       ...        93.000000  93.000000  93.000000  93.000000  93.000000   \n",
       "mean        ...         0.054794   0.065587   0.879199   0.884357   0.879257   \n",
       "std         ...         0.014699   0.041173   0.027835   0.017293   0.038059   \n",
       "min         ...         0.027500   0.023455   0.778887   0.836053   0.762396   \n",
       "25%         ...         0.044750   0.046639   0.860199   0.873145   0.857936   \n",
       "50%         ...         0.053143   0.054647   0.880331   0.885531   0.875645   \n",
       "75%         ...         0.063600   0.068679   0.898970   0.898300   0.901714   \n",
       "max         ...         0.096667   0.318333   0.934839   0.917956   1.000000   \n",
       "\n",
       "           v_pos   v_pos_MD   v_pos_QA  w_mod_perc_abs  w_mod_perc_rel  \n",
       "count  93.000000  93.000000  93.000000       93.000000       93.000000  \n",
       "mean    0.213371   0.153589   0.245192        0.007331        0.072351  \n",
       "std     0.049806   0.021697   0.068170        0.002250        0.018539  \n",
       "min     0.109614   0.108308   0.091524        0.003347        0.034301  \n",
       "25%     0.178993   0.138208   0.199177        0.005526        0.057812  \n",
       "50%     0.214071   0.153585   0.247348        0.007283        0.071942  \n",
       "75%     0.247414   0.166529   0.300054        0.008321        0.084071  \n",
       "max     0.317834   0.212932   0.360683        0.016802        0.129657  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_sent: Get only sentiment+syntax+semantic Indicators\n",
    "import numpy as np\n",
    "# Get only readability predictors:\n",
    "data_sent= data[[ 'Polarity', 'Polarity_QA', 'Subjectivity', 'Subjectivity_QA',\n",
    "                 'neg_perc_abs', 'neg_perc_rel', 'pos_perc_abs', 'pos_perc_rel','unc_perc_abs', 'unc_perc_rel',\n",
    "                 'v_comp', 'v_comp_MD', 'v_comp_QA', 'v_neg', 'v_neg_MD', 'v_neg_QA', 'v_neu', 'v_neu_MD', 'v_neu_QA',\n",
    "                 'v_pos', 'v_pos_MD', 'v_pos_QA', 'w_mod_perc_abs', 'w_mod_perc_rel'\n",
    "                ]]\n",
    "\n",
    "data_sent.fillna(np.nanmean(data_sent),inplace=True)\n",
    "len(data_sent.columns) # 24 numerical predictors\n",
    "data_sent.describe() # you need to scale first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_t = pd.concat([data_gr,data_sent], axis=1) # Total Sample Readability + Sentiment dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML - Selected Models - Minimazing Misc. Error and Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ensemble results below so that you don't have to train models again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ML_Opt = pd.read_csv('ML_Ensem_Optimal.csv') # Ensemble Optimal=> Min GE and Log-Loss\n",
    "ML_Logit_RF_Tot = pd.read_csv('ML_Ensem_Logit_RF_Tot.csv') # Ensemble Logit and RF using Total Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models selected for ensemble right below are the ones with better balance Minimum Classification Error and Minimum Generalization Error:\n",
    "\n",
    "- **XGB Text**: uses data_gr\n",
    "- **PCA Logit Total**: uses df_t\n",
    "- **RF Total**: uses df_t\n",
    "- **PCA Logit Syn/Sem**: uses data_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB Text**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uses data_gr\n",
    "import sklearn.cross_validation as cv\n",
    "\n",
    "pred = data_gr\n",
    "target = data['Target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(pred, target, test_size=0.20, random_state=0) # Test is 20% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [1000, 5000, 10000], 'learning_rate': [0.01, 0.005], 'colsample_bytree': [0.8], 'max_depth': [2, 3], 'nthread': [16]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune XGB model to find the best model:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import sklearn.grid_search as gs\n",
    "\n",
    "#label_enc_y = LabelEncoder().fit_transform(y_train)\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Important: EXCLUDE 'objective' IN THE PARAMETERS BELOW FOR sklearn.xgboost to work properly\n",
    "# http://stackoverflow.com/questions/35384977/xgbclassifier-num-class-is-invalid\n",
    "\n",
    "grid_para = [{ 'max_depth':[2,3], # max tree depth for base learners \n",
    "              'learning_rate':[0.01,0.005], # = eta\n",
    "              'n_estimators':[1000, 5000, 10000], # num of boosted trees to fit\n",
    "              'nthread':[16],\n",
    "              'colsample_bytree':[0.8]\n",
    "             }]\n",
    "\n",
    "grid_xgb = gs.GridSearchCV(xgb_model, grid_para, scoring='accuracy', cv=5)\n",
    "grid_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'learning_rate': 0.005,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 5000,\n",
       " 'nthread': 16}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Parameters\n",
    "grid_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Score 1.0\n",
      "Training Error 0.0\n",
      "Overall Training Log-Loss Error 0.0408724087913\n",
      "*****************************************************************\n",
      "Test Score 0.842105263158\n",
      "Test Error 0.157894736842\n",
      "Overall Test Log-Loss Error 0.527723529866\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print 'Training Score',grid_xgb.best_estimator_.score(x_train,y_train)\n",
    "print 'Training Error',1 - grid_xgb.best_estimator_.score(x_train,y_train)\n",
    "y_train_p = grid_xgb.best_estimator_.predict_proba(x_train)\n",
    "print 'Overall Training Log-Loss Error', metrics.log_loss(y_train,y_train_p)\n",
    "print '*'*65\n",
    "print 'Test Score',grid_xgb.best_estimator_.score(x_test,y_test)\n",
    "print 'Test Error',1 - grid_xgb.best_estimator_.score(x_test,y_test)\n",
    "y_test_p = grid_xgb.best_estimator_.predict_proba(x_test)\n",
    "print 'Overall Test Log-Loss Error', metrics.log_loss(y_test,y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Input\n",
    "y_xgb_train_p = grid_xgb.best_estimator_.predict(x_train)\n",
    "y_xgb_train_prob = grid_xgb.best_estimator_.predict_proba(x_train)\n",
    "\n",
    "y_xgb_test_p = grid_xgb.best_estimator_.predict(x_test)\n",
    "y_xgb_test_prob = grid_xgb.best_estimator_.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Logit Total**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAF9CAYAAAAjuOMbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWW04cTEzM7Pa+C8x+5kD1hPb+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9de4240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "df_t_s = df_t / np.std(df_t,0) # scaling for PCA\n",
    "\n",
    "# Checking what number of PCs are best to wok with: \n",
    "pca.set_params(n_components = None)\n",
    "pca.fit(df_t_s)\n",
    "plt.plot(range(len(df_t_s.columns)), pca.explained_variance_ratio_)\n",
    "plt.scatter(range(len(df_t_s.columns)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('ith components')\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Explained Variance 95.0\n",
      "*****************************************************************\n",
      "Explained Variance per  PC:\n",
      "*****************************************************************\n",
      "[ 0.23130789  0.18092125  0.12761007  0.08097086  0.06902264  0.06169577\n",
      "  0.03537737  0.03041254  0.02749783  0.02269523  0.0209422   0.01597056\n",
      "  0.01401388  0.01110328  0.00944165  0.00866352]\n",
      "*****************************************************************\n",
      "PCA Eigenvectors\n",
      "*****************************************************************\n",
      "[[-0.1998068  -0.19680465 -0.20006277 ..., -0.03305734  0.07196311\n",
      "   0.06304699]\n",
      " [ 0.13098076  0.13617806  0.12373762 ..., -0.0410448   0.03932919\n",
      "   0.00886406]\n",
      " [-0.10105844 -0.10368048 -0.11571214 ...,  0.18744966 -0.0599132\n",
      "  -0.05938558]\n",
      " ..., \n",
      " [-0.01348954 -0.01642016 -0.01171516 ...,  0.0182919  -0.04395483\n",
      "  -0.01962819]\n",
      " [ 0.01109968  0.01701681  0.01005861 ..., -0.00614481 -0.1786937\n",
      "  -0.22796943]\n",
      " [-0.02400469 -0.02163471 -0.01315836 ...,  0.04890901 -0.16725797\n",
      "  -0.19261005]]\n",
      "*****************************************************************\n",
      "PCs\n",
      "*****************************************************************\n",
      "[[-2.24472768  2.91689675  2.27174302 ...,  0.79644664 -0.32862393\n",
      "   0.3249574 ]\n",
      " [ 0.74077051 -2.74377317 -1.84272791 ...,  0.76081649 -0.31630535\n",
      "  -0.27826452]\n",
      " [-1.969855   -1.51245379 -0.92070995 ...,  0.79415845  0.26079228\n",
      "  -0.46029325]\n",
      " ..., \n",
      " [ 0.05285996 -0.89133425  0.98682075 ...,  0.98855251  0.56853214\n",
      "  -0.06849885]\n",
      " [-1.90657914 -1.6068661   1.00389789 ..., -0.31862617  1.13441088\n",
      "   0.97416057]\n",
      " [ 0.00926262 -1.67459239  1.10790196 ...,  0.59606352  0.34197667\n",
      "   0.36980761]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.set_params(n_components = 16) # only obtain 3 PCs = p\n",
    "pca.fit(df_t_s)\n",
    "\n",
    "print 'Aggregated Explained Variance', 100*round(sum(pca.explained_variance_ratio_),2)\n",
    "print '*'*65\n",
    "print 'Explained Variance per  PC:'\n",
    "print '*'*65\n",
    "print pca.explained_variance_ratio_\n",
    "print '*'*65\n",
    "print 'PCA Eigenvectors'\n",
    "print '*'*65\n",
    "print pca.components_ # WARNING: these are eigenvectors, not PCs => remember PCs live in the obs space so 3x3 matrix doesn't make sense for PCs\n",
    "print '*'*65\n",
    "print 'PCs'\n",
    "print '*'*65\n",
    "print pca.transform(df_t_s) # WARNING: these are eigenvectors, not PCs => remember PCs live in the obs space so 3x3 matrix doesn't make sense for PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.1486\n",
      "The test error is: 0.1579\n",
      "Training Multi Class Log_loss: 0.342965474627\n",
      "Test Multi Class Log_loss: 0.358668915309\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation as cv\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "logit = lm.LogisticRegression()\n",
    "\n",
    "pred = df_t_s\n",
    "target = data['Target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(pred, target, test_size=0.20, random_state=0) # Test is 20% data\n",
    "\n",
    "# PCA Training\n",
    "pca.set_params(n_components = 17) # only obtain 16 PCs out of 66\n",
    "pca.fit(x_train)\n",
    "x_train = pca.transform(x_train)  \n",
    "x_test = pca.transform(x_test)  \n",
    "\n",
    "# Logit \n",
    "logit.fit(x_train, y_train)\n",
    "print \"The training error is: %.4f\" %(1-logit.score(x_train, y_train))\n",
    "print \"The test error is: %.4f\" %(1-logit.score(x_test, y_test))\n",
    "y_train_p = logit.predict_proba(x_train)\n",
    "y_test_p = logit.predict_proba(x_test)\n",
    "print 'Training Multi Class Log_loss:', metrics.log_loss(y_train,y_train_p)\n",
    "print 'Test Multi Class Log_loss:', metrics.log_loss(y_test,y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Input\n",
    "y_logit_train_p = logit.predict(x_train)\n",
    "y_logit_train_prob = logit.predict_proba(x_train)\n",
    "\n",
    "y_logit_test_p = logit.predict(x_test)\n",
    "y_logit_test_prob = logit.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RF Total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split Train/Test data:\n",
    "import sklearn.cross_validation as cv\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "pred = df_t\n",
    "target = data['Target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(pred, target, test_size=0.20, random_state=0) # Test is 20% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [10, 50, 100], 'min_samples_split': array([  2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,  22.,\n",
       "        24.,  26.,  28.,  30.]), 'criterion': ['gini', 'entropy'], 'max_features': ['auto', 'log2', 10], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune RF model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import sklearn.grid_search as gs\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "np.random.seed(1)\n",
    "\n",
    "grid_para_forest = [{\"n_estimators\": [10, 50, 100], \"criterion\": [\"gini\", \"entropy\"], \\\n",
    "                    \"min_samples_leaf\": range(1, 10), \"min_samples_split\": np.linspace(2, 30, 15), \\\n",
    "                    'max_features':['auto','log2',10]}]\n",
    "grid_search_forest = gs.GridSearchCV(randomForest, grid_para_forest, scoring='accuracy', cv=5)\n",
    "grid_search_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4.0,\n",
       " 'n_estimators': 50}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## best parameter\n",
    "grid_search_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.878378378378\n",
      "Best Error: 0.121621621622\n",
      "*****************************************************************\n",
      "Overall Training Score: 1.0\n",
      "Overall Training Error 0.0\n",
      "Overall Training Log-Loss Error 0.156279808886\n",
      "*****************************************************************\n",
      "Overall Test Score: 0.789473684211\n",
      "Overall Test Error 0.210526315789\n",
      "Overall Test Log-Loss Error 0.416160069888\n"
     ]
    }
   ],
   "source": [
    "## best and overall score\n",
    "print 'Best Score:',grid_search_forest.best_score_\n",
    "print 'Best Error:',1- grid_search_forest.best_score_\n",
    "print 65*'*'\n",
    "print 'Overall Training Score:', grid_search_forest.score(x_train,y_train)  \n",
    "print 'Overall Training Error', 1-grid_search_forest.score(x_train,y_train)\n",
    "y_train_p = grid_search_forest.predict_proba(x_train)\n",
    "print 'Overall Training Log-Loss Error', metrics.log_loss(y_train,y_train_p)\n",
    "print 65*'*'\n",
    "print 'Overall Test Score:', grid_search_forest.score(x_test,y_test)  \n",
    "print 'Overall Test Error', 1-grid_search_forest.score(x_test,y_test)\n",
    "y_test_p = grid_search_forest.predict_proba(x_test)\n",
    "print 'Overall Test Log-Loss Error', metrics.log_loss(y_test,y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=4.0,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Model Training:\n",
    "randomForest = RandomForestClassifier(criterion='gini',max_features= 'auto', min_samples_leaf= 1, \\\n",
    "                                               min_samples_split= 4.0, n_estimators= 50)\n",
    "randomForest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Score: 1.0\n",
      "Best Training Error: 0.0\n",
      "Overall Train Log-Loss Error 0.16213222578\n",
      "Best Test Score: 0.789473684211\n",
      "Best Test Error: 0.210526315789\n",
      "Overall Test Log-Loss Error 0.420290820315\n"
     ]
    }
   ],
   "source": [
    "print 'Best Training Score:',randomForest.score(x_train,y_train)\n",
    "print 'Best Training Error:',1- randomForest.score(x_train,y_train)\n",
    "y_train_p = randomForest.predict_proba(x_train)\n",
    "print 'Overall Train Log-Loss Error', metrics.log_loss(y_train,y_train_p)\n",
    "\n",
    "print 'Best Test Score:',randomForest.score(x_test,y_test)\n",
    "print 'Best Test Error:',1- randomForest.score(x_test,y_test)\n",
    "y_test_p = randomForest.predict_proba(x_test)\n",
    "print 'Overall Test Log-Loss Error', metrics.log_loss(y_test,y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Input\n",
    "# Ensemble Input\n",
    "y_rf_train_p = randomForest.predict(x_train)\n",
    "y_rf_train_prob = randomForest.predict_proba(x_train)\n",
    "\n",
    "y_rf_test_p = randomForest.predict(x_test)\n",
    "y_rf_test_prob = randomForest.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PCA Logit Syn/Sem **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAF9CAYAAAAjuOMbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIaihwERERkdT4f1shxPU1yO6ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x142b5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "data_sent_s = data_sent / np.std(data_sent,0) # scaling for PCA\n",
    "\n",
    "# Checking what number of PCs are best to wok with: \n",
    "pca.set_params(n_components = None)\n",
    "pca.fit(data_sent_s)\n",
    "plt.plot(range(len(data_sent_s.columns)), pca.explained_variance_ratio_)\n",
    "plt.scatter(range(len(data_sent_s.columns)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('ith components')\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Explained Variance 97.0\n",
      "*****************************************************************\n",
      "Explained Variance per  PC:\n",
      "*****************************************************************\n",
      "[ 0.29093718  0.18858639  0.1295608   0.07894691  0.06509019  0.05566414\n",
      "  0.04966892  0.03478698  0.02764545  0.02016397  0.01333225  0.01217238]\n",
      "*****************************************************************\n",
      "PCA Eigenvectors\n",
      "*****************************************************************\n",
      "[[ -2.72890483e-01  -2.32800580e-01   1.79778425e-01   1.56133565e-01\n",
      "    2.63248547e-01   2.63840858e-01  -2.48377096e-01  -2.93735102e-01\n",
      "    4.61276439e-02   2.95441609e-02  -2.88536980e-01  -2.80918684e-01\n",
      "   -2.37584087e-01   1.41807346e-01   2.11190257e-01   1.19516380e-01\n",
      "    1.99809778e-01   1.91230542e-01   1.93070503e-01  -2.04614390e-01\n",
      "   -2.03748579e-01  -1.65148468e-01   4.06448099e-02   3.26658027e-02]\n",
      " [ -1.92094426e-01  -2.05248202e-02   7.11467703e-02  -1.71308292e-01\n",
      "    1.93759301e-01   2.88996471e-01  -1.28504787e-01  -1.78593422e-02\n",
      "   -3.35105931e-01  -3.16845798e-01  -1.56808921e-01  -1.27353748e-01\n",
      "   -1.06173285e-01  -7.49033921e-03  -5.46085228e-02  -9.02057069e-02\n",
      "   -2.72388097e-01  -1.25855034e-01  -3.07486501e-01   2.57191372e-01\n",
      "    5.54994907e-02   2.46316077e-01  -3.25050701e-01  -3.07071388e-01]\n",
      " [  6.38375640e-03   1.14915062e-01   8.28359793e-02  -2.17420056e-01\n",
      "    5.28845730e-02   7.62411000e-02  -2.65451023e-02   5.98579619e-03\n",
      "   -2.72793669e-01  -3.04191950e-01   2.00310560e-01   9.27005820e-02\n",
      "    2.64463192e-01  -2.58879374e-01  -9.78513925e-02  -3.47934224e-01\n",
      "    3.05212870e-01   1.86547759e-01   2.36246900e-01  -2.71851499e-01\n",
      "   -1.99929201e-01  -2.97952464e-01  -1.59029064e-01  -1.68234562e-01]\n",
      " [ -2.79267064e-01  -2.05994962e-01  -1.01550784e-01   4.44357430e-02\n",
      "    6.16182210e-02   3.89233530e-02  -2.48963185e-01  -3.04163709e-01\n",
      "    1.62451144e-01   1.68290381e-01   8.20537517e-02   6.23772757e-02\n",
      "    1.02121985e-01  -4.51311486e-01  -3.19274892e-01  -3.69252235e-01\n",
      "   -8.42784813e-02   1.27731843e-01  -9.07473773e-02   1.82566965e-01\n",
      "    2.30699861e-02   1.58923642e-01   2.18369935e-01   2.26472915e-01]\n",
      " [ -4.71111995e-03  -2.41968890e-01   4.33654098e-01   3.14416442e-01\n",
      "    2.74353368e-01   8.15350669e-02   2.93184387e-01   5.44919484e-02\n",
      "    2.25293717e-01   5.27618907e-02   8.71263542e-02   1.56332577e-01\n",
      "   -4.53736068e-02  -1.88324084e-01  -1.65974208e-01  -3.26608047e-02\n",
      "    1.67543215e-02  -2.86094575e-01   1.26143773e-01  -9.03079997e-02\n",
      "    3.47760780e-01  -9.17772245e-02  -8.99467598e-02  -2.97171011e-01]\n",
      " [  1.01233768e-01  -2.15411911e-01  -2.64550576e-01   4.39836664e-01\n",
      "   -2.32906666e-01  -3.27624303e-02  -6.05729513e-02   2.19858052e-01\n",
      "   -2.76577575e-02   1.58143084e-01  -3.31005101e-02  -2.05756169e-02\n",
      "   -1.24604928e-01  -1.91283101e-01  -1.84742684e-01   9.29570925e-02\n",
      "    2.27509498e-02   2.40137010e-01   9.30469004e-02  -5.09863353e-04\n",
      "   -3.35993000e-01   1.75128165e-01  -3.96167559e-01  -2.88183340e-01]\n",
      " [ -1.65446465e-01  -3.62916640e-01  -3.85302817e-01  -1.46290988e-01\n",
      "   -1.84173325e-01  -5.65335122e-02  -3.40438315e-01  -1.87801120e-01\n",
      "   -3.67934545e-02   8.51426494e-02   1.53783221e-01   2.08808030e-01\n",
      "    5.80232101e-02   1.37964582e-01   1.06301837e-01   7.23629010e-02\n",
      "   -1.40069081e-02  -3.95307163e-01   6.70330615e-02  -1.70390986e-01\n",
      "    2.23570894e-01  -2.47879859e-01  -2.22456048e-01  -1.30691577e-01]\n",
      " [ -1.06991548e-01  -9.38005769e-02   3.50014012e-02   1.96404182e-02\n",
      "    1.41097523e-01   1.01339985e-01  -1.64134079e-02  -4.26551389e-02\n",
      "   -1.29554859e-01  -1.95158512e-01   2.14251980e-01   4.28588758e-01\n",
      "   -2.05623536e-02   3.27074554e-01  -5.23975572e-01   4.37463878e-01\n",
      "   -2.28149645e-02   2.20240737e-01  -5.98059360e-02  -2.96915857e-02\n",
      "   -9.68271978e-02  -5.46978634e-04   1.23761531e-01   9.95465970e-02]\n",
      " [  6.82235858e-02   2.49111311e-01   2.02751903e-01  -4.91302567e-02\n",
      "    1.23885213e-01  -1.34465527e-02  -1.76916998e-01  -3.02563470e-01\n",
      "    3.23651685e-01   3.04816446e-01   2.45516849e-01  -2.84747095e-02\n",
      "    4.33342749e-01   1.59546585e-01   9.05630636e-02   1.30740380e-01\n",
      "   -1.01513849e-01   1.47616103e-01  -1.80303230e-01   3.10094856e-02\n",
      "   -1.63772804e-01   6.09009321e-02  -2.15166820e-01  -3.34233655e-01]\n",
      " [ -2.72363290e-01  -3.63874498e-01  -6.25826614e-02   2.13004618e-01\n",
      "    5.10585927e-02   3.06149210e-02   2.24346320e-01   1.54234002e-01\n",
      "   -1.02739268e-01  -2.14457400e-01   2.66133962e-01  -1.11057395e-01\n",
      "    5.06904161e-01   9.65031368e-02   3.75093389e-01   5.20035986e-02\n",
      "    4.32345903e-02   3.41602694e-02   8.49427321e-02   1.59160389e-01\n",
      "   -5.64138672e-02   2.47320772e-01   1.09913471e-01   7.12418138e-02]\n",
      " [  1.18278906e-01  -3.30802461e-01   3.21952960e-01  -1.77233639e-01\n",
      "   -8.09291283e-02  -1.68471464e-01   4.85232478e-02  -5.46432244e-02\n",
      "    4.35842510e-02  -1.70019053e-02   6.25135086e-02   3.62484849e-01\n",
      "   -2.40250770e-01   4.98973086e-02   2.32672119e-01  -2.31844408e-01\n",
      "   -1.19486217e-01  -1.72317378e-01  -1.36723213e-01  -5.73188069e-02\n",
      "   -5.52265089e-01   1.46530468e-01   5.43086998e-02   1.68932880e-02]\n",
      " [  1.31480365e-01   4.19528245e-02   1.02359498e-01   3.83809640e-01\n",
      "   -8.06982998e-02  -2.67718185e-02  -1.41743681e-01  -4.98033941e-02\n",
      "   -1.74818265e-01  -1.73681364e-01   2.65956715e-02  -2.39828728e-01\n",
      "    2.07974004e-01  -1.70212850e-01  -7.62030619e-02   1.08578046e-01\n",
      "   -4.32594531e-01  -2.48554690e-01  -2.63073349e-01  -3.64081810e-01\n",
      "   -1.70039176e-01  -2.50005975e-01   1.16086825e-01   1.70347908e-01]]\n",
      "*****************************************************************\n",
      "PCs\n",
      "*****************************************************************\n",
      "[[ -9.14135313e-01  -1.88466435e+00  -1.08962045e+00 ...,  -3.06590560e-01\n",
      "    1.22360030e-01  -3.95535166e-01]\n",
      " [  3.53091860e+00   1.52257632e+00  -6.77451033e-01 ...,   6.43466142e-01\n",
      "   -1.28638763e+00  -1.00505787e-01]\n",
      " [ -1.49226831e+00  -6.13585364e-01   2.16810350e+00 ...,  -1.67231783e-03\n",
      "   -1.39088660e+00   6.59809649e-01]\n",
      " ..., \n",
      " [  4.58503827e-01  -6.51052338e-01  -1.72981571e+00 ...,   3.70152627e-01\n",
      "   -5.28168365e-01  -3.46226482e-01]\n",
      " [ -7.11542160e-01   1.84848241e+00   1.77460432e+00 ...,   1.35468147e+00\n",
      "   -9.03966928e-01  -5.32809600e-02]\n",
      " [  2.15948482e+00   2.23038636e+00  -8.46360253e-01 ...,   7.49759521e-01\n",
      "   -7.17606512e-01   4.98831799e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.set_params(n_components = 12) # only obtain 3 PCs = p\n",
    "pca.fit(data_sent_s)\n",
    "\n",
    "print 'Aggregated Explained Variance', 100*round(sum(pca.explained_variance_ratio_),2)\n",
    "print '*'*65\n",
    "print 'Explained Variance per  PC:'\n",
    "print '*'*65\n",
    "print pca.explained_variance_ratio_\n",
    "print '*'*65\n",
    "print 'PCA Eigenvectors'\n",
    "print '*'*65\n",
    "print pca.components_ # WARNING: these are eigenvectors, not PCs => remember PCs live in the obs space so 3x3 matrix doesn't make sense for PCs\n",
    "print '*'*65\n",
    "print 'PCs'\n",
    "print '*'*65\n",
    "print pca.transform(data_sent_s) # WARNING: these are eigenvectors, not PCs => remember PCs live in the obs space so 3x3 matrix doesn't make sense for PCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.1486\n",
      "The test error is: 0.2105\n",
      "Training Multi Class Log_loss: 0.398282697455\n",
      "Test Multi Class Log_loss: 0.541899051837\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation as cv\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "logit = lm.LogisticRegression()\n",
    "\n",
    "pred = data_sent_s\n",
    "target = data['Target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(pred, target, test_size=0.20, random_state=0) # Test is 20% data\n",
    "\n",
    "# PCA Training\n",
    "pca.set_params(n_components = 12) # only obtain 7 PCs => from our previous analysis\n",
    "pca.fit(x_train)\n",
    "x_train = pca.transform(x_train) # Use 7 PCs as predictors to reduce complexity from the original 36 predictors \n",
    "x_test = pca.transform(x_test) # Use 7 PCs as predictors to reduce complexity from the original 36 predictors \n",
    "\n",
    "# Logit \n",
    "logit.fit(x_train, y_train)\n",
    "print \"The training error is: %.4f\" %(1-logit.score(x_train, y_train))\n",
    "print \"The test error is: %.4f\" %(1-logit.score(x_test, y_test))\n",
    "y_train_p = logit.predict_proba(x_train)\n",
    "y_test_p = logit.predict_proba(x_test)\n",
    "print 'Training Multi Class Log_loss:', metrics.log_loss(y_train,y_train_p)\n",
    "print 'Test Multi Class Log_loss:', metrics.log_loss(y_test,y_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Error rates per fold: [ 0.13333333  0.2         0.2         0.26666667  0.21428571]\n",
      "CV Average Error 0.202857142857\n",
      "CV Error Std. Dev 0.0425491579118\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "import sklearn.cross_validation as cv\n",
    "stratify_divide = cv.StratifiedKFold(y_train, 5, random_state=0)\n",
    "scores = cv.cross_val_score(logit, x_train, y_train, cv=stratify_divide)\n",
    "print 'CV Error rates per fold:', 1- scores\n",
    "print 'CV Average Error', 1-np.mean(scores)\n",
    "print 'CV Error Std. Dev', np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Input\n",
    "y_logitsent_train_p = logit.predict(x_train)\n",
    "y_logitsent_train_prob = logit.predict_proba(x_train)\n",
    "\n",
    "y_logitsent_test_p = logit.predict(x_test)\n",
    "y_logitsent_test_prob = logit.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model 1: Soft  = Majority Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Train Score 0.98649\n",
      "Ensemble Train Error 0.01351\n",
      "Ensemble Train Log-Loss 0.20117\n"
     ]
    }
   ],
   "source": [
    "# Train Ensemble\n",
    "y_target = y_train\n",
    "\n",
    "y_logit_prob = y_logit_train_prob\n",
    "y_rf_prob = y_rf_train_prob\n",
    "y_xgb_prob = y_xgb_train_prob\n",
    "y_logitsent_prob = y_logitsent_train_prob\n",
    "\n",
    "\n",
    "df_pred = pd.DataFrame({'0_logit':y_logit_prob[:,0], '1_logit':y_logit_prob[:,1],\n",
    "                        '0_rf': y_rf_prob[:,0], '1_rf': y_rf_prob[:,1], \n",
    "                        '0_xgb':y_xgb_prob[:,0], '1_xgb':y_xgb_prob[:,1],\n",
    "                        '0_logitsent':y_logitsent_prob[:,0], '1_logitsent':y_logitsent_prob[:,1]\n",
    "                       })\n",
    "df_pred['0_avg']= np.average(df_pred[['0_logit','0_rf','0_xgb','0_logitsent']],axis=1)\n",
    "df_pred['1_avg']= np.average(df_pred[['1_logit','1_rf','1_xgb','1_logitsent']],axis=1)\n",
    "df_pred['Pred']= [0 if list(df_pred['0_avg'])[i] > list(df_pred['1_avg'])[i] else 1 for i in range(0,len(df_pred),1)]\n",
    "\n",
    "score = float(sum(y_target==df_pred['Pred']))/len(y_target)\n",
    "error = 1- score\n",
    "\n",
    "print 'Ensemble Train Score', round(score,5)\n",
    "print 'Ensemble Train Error', round(1 - score,5)\n",
    "print 'Ensemble Train Log-Loss', round(metrics.log_loss(y_target,np.array(df_pred[['0_avg','1_avg']])),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test Score 0.84211\n",
      "Ensemble Test Error 0.15789\n",
      "Ensemble Test Log-Loss 0.36726\n"
     ]
    }
   ],
   "source": [
    "# Test Ensemble\n",
    "y_target = y_test\n",
    "\n",
    "y_logit_prob = y_logit_test_prob\n",
    "y_rf_prob = y_rf_test_prob\n",
    "y_xgb_prob = y_xgb_test_prob\n",
    "y_logitsent_prob = y_logitsent_test_prob\n",
    "\n",
    "\n",
    "df_pred = pd.DataFrame({'0_logit':y_logit_prob[:,0], '1_logit':y_logit_prob[:,1],\n",
    "                        '0_rf': y_rf_prob[:,0], '1_rf': y_rf_prob[:,1], \n",
    "                        '0_xgb':y_xgb_prob[:,0], '1_xgb':y_xgb_prob[:,1],\n",
    "                        '0_logitsent':y_logitsent_prob[:,0], '1_logitsent':y_logitsent_prob[:,1],\n",
    "                       })\n",
    "df_pred['0_avg']= np.average(df_pred[['0_logit','0_rf','0_xgb','0_logitsent']],axis=1)\n",
    "df_pred['1_avg']= np.average(df_pred[['1_logit','1_rf','1_xgb','1_logitsent']],axis=1)\n",
    "df_pred['Pred']= [0 if list(df_pred['0_avg'])[i] > list(df_pred['1_avg'])[i] else 1 for i in range(0,len(df_pred),1)]\n",
    "\n",
    "score = float(sum(y_target==df_pred['Pred']))/len(y_target)\n",
    "error = 1- score\n",
    "\n",
    "\n",
    "print 'Ensemble Test Score', round(score,5)\n",
    "print 'Ensemble Test Error', round(1 - score,5)\n",
    "print 'Ensemble Test Log-Loss', round(metrics.log_loss(y_target,np.array(df_pred[['0_avg','1_avg']])),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model 2: Soft  = Majority Rule - Only PCA Logit Total and RF Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Train Score 0.95946\n",
      "Ensemble Train Error 0.04054\n",
      "Ensemble Train Log-Loss 0.22908\n"
     ]
    }
   ],
   "source": [
    "# Train Ensemble\n",
    "y_target = y_train\n",
    "\n",
    "y_logit_prob = y_logit_train_prob\n",
    "y_rf_prob = y_rf_train_prob\n",
    "\n",
    "\n",
    "df_pred = pd.DataFrame({'0_logit':y_logit_prob[:,0], '1_logit':y_logit_prob[:,1],\n",
    "                        '0_rf': y_rf_prob[:,0], '1_rf': y_rf_prob[:,1]                    \n",
    "                       })\n",
    "df_pred['0_avg']= np.average(df_pred[['0_logit','0_rf']],axis=1)\n",
    "df_pred['1_avg']= np.average(df_pred[['1_logit','1_rf',]],axis=1)\n",
    "df_pred['Pred']= [0 if list(df_pred['0_avg'])[i] > list(df_pred['1_avg'])[i] else 1 for i in range(0,len(df_pred),1)]\n",
    "\n",
    "score = float(sum(y_target==df_pred['Pred']))/len(y_target)\n",
    "error = 1- score\n",
    "\n",
    "print 'Ensemble Train Score', round(score,5)\n",
    "print 'Ensemble Train Error', round(1 - score,5)\n",
    "print 'Ensemble Train Log-Loss', round(metrics.log_loss(y_target,np.array(df_pred[['0_avg','1_avg']])),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test Score 0.78947\n",
      "Ensemble Test Error 0.21053\n",
      "Ensemble Test Log-Loss 0.37747\n"
     ]
    }
   ],
   "source": [
    "# Test Ensemble\n",
    "y_target = y_test\n",
    "\n",
    "y_logit_prob = y_logit_test_prob\n",
    "y_rf_prob = y_rf_test_prob\n",
    "\n",
    "\n",
    "df_pred = pd.DataFrame({'0_logit':y_logit_prob[:,0], '1_logit':y_logit_prob[:,1],\n",
    "                        '0_rf': y_rf_prob[:,0], '1_rf': y_rf_prob[:,1]                       })\n",
    "\n",
    "df_pred['0_avg']= np.average(df_pred[['0_logit','0_rf']],axis=1)\n",
    "df_pred['1_avg']= np.average(df_pred[['1_logit','1_rf']],axis=1)\n",
    "df_pred['Pred']= [0 if list(df_pred['0_avg'])[i] > list(df_pred['1_avg'])[i] else 1 for i in range(0,len(df_pred),1)]\n",
    "\n",
    "score = float(sum(y_target==df_pred['Pred']))/len(y_target)\n",
    "error = 1- score\n",
    "\n",
    "\n",
    "print 'Ensemble Test Score', round(score,5)\n",
    "print 'Ensemble Test Error', round(1 - score,5)\n",
    "print 'Ensemble Test Log-Loss', round(metrics.log_loss(y_target,np.array(df_pred[['0_avg','1_avg']])),5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
